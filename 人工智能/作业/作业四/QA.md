# QA

>   将机器人寻路问题简化为图1的2*2的网格，假设有位于s1位置的机器人拟从s1这一初始位置向s4这一 目标位置移动。机器人每次只能向上或者向右移动一个方格，到达目标位置s4则会获得奖励且游戏终 止，机器人在移动过程中如果越出方格(sd)则会被惩罚且被损坏、并且游戏终止。奖励值定义如下: 当St+1 = s4时奖励值为1，当St+1 = sd时惩罚值为-1，其他情况下奖励值为0。若折扣因子γ=0.99， 智能体在s1、s2、s3的策略都初始化为上，终止状态s4、sd的价值函数定义为0，试通过联立贝尔曼方程给出状态s1 、s2 、s3 的价值函数。

明确各个状态的价值函数定义和策略：

- $s1$ 的策略是向上移动到 $s3$
- $s2$ 的策略是向上移动到 $sd$ （越界）
- $s3$ 的策略是向上移动到 $s4$
- $s4$ 是终止状态，价值函数为 0
- $sd$ 是越界状态，价值函数为 0

根据贝尔曼方程，状态 $s_i$ 的价值函数 $V(s_i)$ 可以表示为：

$$
V(s_i) = R(s_i, a) + \gamma \sum_{s'} P(s'|s_i, a) V(s')
$$

其中：
- $R(s_i, a)$ 是在状态 $s_i$ 执行动作 $a$ 获得的即时奖励
- $\gamma$ 是折扣因子
- $P(s'|s_i, a)$ 是在状态 $s_i$ 执行动作 $a$ 转移到状态 $s'$ 的概率

1. **状态 $s1$**:
   - 策略是向上移动到 $s3$
   - 即时奖励 $R(s1, \text{上}) = 0$
   - 转移到 $s3$ 的概率是 1

   $$
   V(s1) = R(s1, \text{上}) + \gamma V(s3)
   $$
   $$
   V(s1) = 0 + 0.99 V(s3)
   $$
   $$
   V(s1) = 0.99 V(s3)
   $$

2. **状态 $s2$**:
   - 策略是向上移动到越界状态 $sd$
   - 即时奖励 $R(s2, \text{上}) = -1$
   - 转移到 $sd$ 的概率是 1

   $$
   V(s2) = R(s2, \text{上}) + \gamma V(sd)
   $$
   $$
   V(s2) = -1 + 0.99 \times 0
   $$
   $$
   V(s2) = -1
   $$

3. **状态 $s3$**:
   - 策略是向上移动到 $s4$
   - 即时奖励 $R(s3, \text{上}) = 1$
   - 转移到 $s4$ 的概率是 1

   $$
   V(s3) = R(s3, \text{上}) + \gamma V(s4)
   $$
   $$
   V(s3) = 1 + 0.99 \times 0
   $$
   $$
   V(s3) = 1
   $$

将 $V(s3)$ 代入 $V(s1)$ 的方程：

$$
V(s1) = 0.99 V(s3)
$$
$$
V(s1) = 0.99 \times 1
$$
$$
V(s1) = 0.99
$$

因此，通过联立贝尔曼方程，我们得到：

- $V(s1) = 0.99$
- $V(s2) = -1$
- $V(s3) = 1$

>   在题1中，若每个状态的价值函数都初始化为0，智能体在s 1 、s 2 、s 3 的策略都初始化为上，试优化智 能体在状态s 3 的策略。（提示：使用策略优化定理）

对于状态 $s$ 的价值函数 $V(s)$，有：

$$
V(s) = \max_{a} Q(s, a)
$$

其中，$Q(s, a)$ 是状态-动作值函数，表示在状态 $s$ 执行动作 $a$ 后的价值。其定义为：

$$
Q(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) V(s')
$$

### 初始化

假设我们已经初始化了所有状态的价值函数为 0：

$$
V(s1) = 0, \quad V(s2) = 0, \quad V(s3) = 0
$$

并且智能体在 $s1$、$s2$ 和 $s3$ 的策略都初始化为向上移动。

### 优化 $s3$ 的策略

需要计算在 $s3$ 的不同策略下的 $Q(s3, a)$，然后选择最大的 $Q(s3, a)$ 对应的动作作为新的策略。

假设在 $s3$ 有两种可行动作：向上和向右。

1. **向上移动到 $s4$**:
   - 即时奖励 $R(s3, \text{上}) = 1$
   - 转移到 $s4$ 的概率是 1
   - $V(s4) = 0$（终止状态，价值函数为 0）

   $$
   Q(s3, \text{上}) = R(s3, \text{上}) + \gamma V(s4)
   $$
   $$
   Q(s3, \text{上}) = 1 + 0.99 \times 0
   $$
   $$
   Q(s3, \text{上}) = 1
   $$

2. **向右移动到越界状态 $sd$**:
   - 即时奖励 $R(s3, \text{右}) = -1$
   - 转移到 $sd$ 的概率是 1
   - $V(sd) = 0$（越界状态，价值函数为 0）

   $$
   Q(s3, \text{右}) = R(s3, \text{右}) + \gamma V(sd)
   $$
   $$
   Q(s3, \text{右}) = -1 + 0.99 \times 0
   $$
   $$
   Q(s3, \text{右}) = -1
   $$

比较这两个 $Q$ 值：

$$
Q(s3, \text{上}) = 1 > Q(s3, \text{右}) = -1
$$

因此，根据策略优化定理，在状态 $s3$，选择向上移动是最优策略。

智能体在状态 $s3$ 的最优策略是向上移动到 $s4$。

>   在题1中， 若图2表示算法的初始状态， 其中a/b表示对应状态的动作-价值函数的取值， 斜线左侧 的a表示q π (s,上)，斜线右侧的b表示q π (s,右)。若α=0.5，试给出Q Learning 算法中的Q学习算法的一 个片段的执行过程，并给出执行完该片段后每个状态的策略

### 初始状态
初始Q值表如下：
```
s1: 0.1/0
s2: 0/0
s3: 0.1/0
s4: 0.1/0
```

其中，斜线左侧的值表示$q(s, \text{上})$，斜线右侧的值表示$q(s, \text{右})$。

假设我们有以下状态转移和奖励信息：
- 在$s1$：
  - 向上移动到$s2$，奖励为1。
  - 向右移动到$s2$，奖励为0。
- 在$s2$：
  - 向上移动到$s3$，奖励为1。
  - 向右移动到$s3$，奖励为0。
- 在$s3$：
  - 向上移动到$s4$，奖励为1。
  - 向右移动到$s4$，奖励为0。
- 在$s4$：
  - 向上移动到终止状态，奖励为1。
  - 向右移动到终止状态，奖励为0。

### Q-Learning 片段执行过程

假设从状态$s1$出发，并执行以下动作序列：
1. 在$s1$选择动作“向上”。
2. 在$s2$选择动作“向右”。
3. 在$s3$选择动作“向上”。
4. 在$s4$选择动作“向上”。

#### 第一步：在$s1$选择动作“向上”
- 当前状态：$s1$。
- 执行动作：向上。
- 转移到状态：$s2$。
- 即时奖励：1。
- 学习率：$\alpha = 0.5$。
- 折扣因子：$\gamma = 0.99$。

更新Q值：
$$
Q(s1, \text{上}) \leftarrow Q(s1, \text{上}) + \alpha \left[ R + \gamma \max_{a'} Q(s2, a') - Q(s1, \text{上}) \right]
$$
$$
Q(s1, \text{上}) \leftarrow 0.1 + 0.5 \left[ 1 + 0.99 \times \max(0, 0) - 0.1 \right]
$$
$$
Q(s1, \text{上}) \leftarrow 0.1 + 0.5 \times (1 - 0.1)
$$
$$
Q(s1, \text{上}) \leftarrow 0.1 + 0.5 \times 0.9
$$
$$
Q(s1, \text{上}) \leftarrow 0.55
$$

更新后的Q值表：
```
s1: 0.55/0
s2: 0/0
s3: 0.1/0
s4: 0.1/0
```

#### 第二步：在$s2$选择动作“向右”
- 当前状态：$s2$。
- 执行动作：向右。
- 转移到状态：$s3$。
- 即时奖励：0。

更新Q值：
$$
Q(s2, \text{右}) \leftarrow Q(s2, \text{右}) + \alpha \left[ R + \gamma \max_{a'} Q(s3, a') - Q(s2, \text{右}) \right]
$$
$$
Q(s2, \text{右}) \leftarrow 0 + 0.5 \left[ 0 + 0.99 \times \max(0.1, 0) - 0 \right]
$$
$$
Q(s2, \text{右}) \leftarrow 0.5 \times 0.099
$$
$$
Q(s2, \text{右}) \leftarrow 0.0495
$$

更新后的Q值表：
```
s1: 0.55/0
s2: 0/0.0495
s3: 0.1/0
s4: 0.1/0
```

#### 第三步：在$s3$选择动作“向上”
- 当前状态：$s3$。
- 执行动作：向上。
- 转移到状态：$s4$。
- 即时奖励：1。

更新Q值：
$$
Q(s3, \text{上}) \leftarrow Q(s3, \text{上}) + \alpha \left[ R + \gamma \max_{a'} Q(s4, a') - Q(s3, \text{上}) \right]
$$
$$
Q(s3, \text{上}) \leftarrow 0.1 + 0.5 \left[ 1 + 0.99 \times \max(0.1, 0) - 0.1 \right]
$$
$$
Q(s3, \text{上}) \leftarrow 0.1 + 0.5 \times (1 + 0.099 - 0.1)
$$
$$
Q(s3, \text{上}) \leftarrow 0.1 + 0.5 \times 0.999
$$
$$
Q(s3, \text{上}) \leftarrow 0.1 + 0.4995
$$
$$
Q(s3, \text{上}) \leftarrow 0.5995
$$

更新后的Q值表：
```
s1: 0.55/0
s2: 0/0.0495
s3: 0.5995/0
s4: 0.1/0
```

#### 第四步：在$s4$选择动作“向上”
- 当前状态：$s4$。
- 执行动作：向上。
- 转移到状态：终止状态。
- 即时奖励：1。

更新Q值：
$$
Q(s4, \text{上}) \leftarrow Q(s4, \text{上}) + \alpha \left[ R + \gamma \max_{a'} Q(\text{终止状态}, a') - Q(s4, \text{上}) \right]
$$
$$
Q(s4, \text{上}) \leftarrow 0.1 + 0.5 \left[ 1 + 0.99 \times 0 - 0.1 \right]
$$
$$
Q(s4, \text{上}) \leftarrow 0.1 + 0.5 \times (1 - 0.1)
$$
$$
Q(s4, \text{上}) \leftarrow 0.1 + 0.5 \times 0.9
$$
$$
Q(s4, \text{上}) \leftarrow 0.55
$$

更新后的Q值表：
```
s1: 0.55/0
s2: 0/0.0495
s3: 0.5995/0
s4: 0.55/0
```

### 策略更新

根据Q值表更新策略，选择每个状态下的最优动作（即Q值最大的动作）：
- $s1$：向上（0.55 > 0）
- $s2$：向右（0.0495 > 0）
- $s3$：向上（0.5995 > 0）
- $s4$：向上（0.55 > 0）

因此，执行完该片段后每个状态的策略为：
- $s1$：向上
- $s2$：向右
- $s3$：向上
- $s4$：向上